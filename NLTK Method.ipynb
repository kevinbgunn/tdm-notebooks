{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile, os, nltk, json, gzip\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tuple_bigrams(tuples_to_convert):\n",
    "    string_grams = []\n",
    "    for tuple_grams in tuples_to_convert:\n",
    "        first_word = tuple_grams[0]\n",
    "        second_word = tuple_grams[1]\n",
    "        gram_string = f'{first_word} {second_word}'\n",
    "        string_grams.append(gram_string)\n",
    "    return string_grams\n",
    "\n",
    "def convert_tuple_trigrams(tuples_to_convert):\n",
    "    string_grams = []\n",
    "    for tuple_grams in tuples_to_convert:\n",
    "        first_word = tuple_grams[0]\n",
    "        second_word = tuple_grams[1]\n",
    "        third_word = tuple_grams[2]\n",
    "        gram_string = f'{first_word} {second_word} {third_word}'\n",
    "        string_grams.append(gram_string)\n",
    "    return string_grams\n",
    "\n",
    "def convert_strings_to_counts(string_grams):\n",
    "    counter_of_grams = Counter(string_grams)\n",
    "    dict_of_grams = dict(counter_of_grams)\n",
    "    return dict_of_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Zip File of Texts\n",
    "import zipfile, os\n",
    "\n",
    "filename = './data/texts.zip'\n",
    "\n",
    "try:\n",
    "    corpus_zip = zipfile. ZipFile(filename)\n",
    "    corpus_zip.extractall('./data/')\n",
    "    corpus_zip.close()\n",
    "    print('Zip file extracted successfully.')\n",
    "except:\n",
    "    print('No zip file detected. Upload your zip file to the data folder.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish root folder holding all text files\n",
    "# Create corpus using all text files in corpus_root\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = './data/texts'\n",
    "corpus = PlaintextCorpusReader(corpus_root, '.*txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus created from: ['120-0.txt', '158-0.txt', '1952-0.txt', '3600-0.txt', '98-0.txt', 'pg514.txt']\n"
     ]
    }
   ],
   "source": [
    "# Print all File IDs in corpus based on text file names\n",
    "text_list = corpus.fileids()\n",
    "print(f'Corpus created from: {text_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 120-0.txt written to json-l file.\n",
      "Text 158-0.txt written to json-l file.\n",
      "Text 1952-0.txt written to json-l file.\n",
      "Text 3600-0.txt written to json-l file.\n",
      "Text 98-0.txt written to json-l file.\n",
      "Text pg514.txt written to json-l file.\n",
      "Process complete. All documents written to json-l file.\n"
     ]
    }
   ],
   "source": [
    "# Compute unigrams, bigrams, trigrams, and wordCount\n",
    "\n",
    "for text in text_list:\n",
    "    \n",
    "    # Compute unigrams\n",
    "    unigrams = corpus.words(text)\n",
    "    unigramCount = convert_strings_to_counts(unigrams)\n",
    "    \n",
    "    # Compute bigrams\n",
    "    tuple_bigrams = list(nltk.bigrams(unigrams))\n",
    "    string_bigrams = convert_tuple_bigrams(tuple_bigrams)\n",
    "    bigramCount = convert_strings_to_counts(string_bigrams)\n",
    "    \n",
    "    # Compute trigrams\n",
    "    tuple_trigrams = list(nltk.trigrams(unigrams))\n",
    "    string_trigrams = convert_tuple_trigrams(tuple_trigrams)\n",
    "    trigramCount = convert_strings_to_counts(string_trigrams)\n",
    "    \n",
    "    # Calculate wordCount\n",
    "    wordCount = len(unigrams)\n",
    "    \n",
    "    # Create a dictionary `data` to hold each document's data\n",
    "    # Including id, wordCount, outputFormat, unigramCount,\n",
    "    # bigramCount, trigramCount, fullText, etc.\n",
    "    data = {}\n",
    "    data.update([\n",
    "        ('id', text),\n",
    "        ('outputFormat', ['unigram', 'bigram', 'trigram', 'fullText']),\n",
    "        ('wordCount', wordCount),\n",
    "        ('fullText', 'placeholder')\n",
    "        ('unigramCount', unigramCount), \n",
    "        ('bigramCount', bigramCount), \n",
    "        ('trigramCount', trigramCount)\n",
    "    ])\n",
    "    \n",
    "    # Write the document to the json file\n",
    "    with open('./data/data.jsonl', 'a') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "        outfile.write('\\n')\n",
    "        print(f'Text {text} written to json-l file.')\n",
    "\n",
    "print('Process complete. All documents written to json-l file.')\n",
    "\n",
    "# GZip dataset\n",
    "\n",
    "f_in = open('./data/data.jsonl', 'rb')\n",
    "f_out = gzip.open('./data/data.jsonl.gz', 'wb')\n",
    "f_out.writelines(f_in)\n",
    "f_out.close()\n",
    "f_in.close()\n",
    "\n",
    "print('Dataset successfully compressed.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
